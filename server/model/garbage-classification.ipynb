{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528efce5-96a3-4c72-b537-e46007fd93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is not meant to be run directly, as many dependencies and data files\n",
    "# are not in the GitHub repository\n",
    "# instead, this file is here to show you the general training process of our\n",
    "# machine-learning model\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import utils, nn, optim\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460b4312-3459-4b31-b3bd-bd959bf22381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters and other variables\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 8\n",
    "\n",
    "RESIZE = 64\n",
    "\n",
    "DATA_AUG = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")  # check cuda or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbdde9a-824e-41c2-9d46-833d205984ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "columns = [\"Label\", \"Path\"]\n",
    "files_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "data_path = \"data/garbage-classification\"\n",
    "categories = os.listdir(data_path)\n",
    "for category in categories:\n",
    "    category_path = data_path + \"/\" + category\n",
    "    files = os.listdir(category_path)\n",
    "    for file in files:\n",
    "        if not file.startswith('.'):\n",
    "            files_data.loc[file] = [category, category_path + \"/\" + file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c7dbed-3e2a-41d0-a4bc-a31179348df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>battery</th>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biological</th>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown-glass</th>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cardboard</th>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothes</th>\n",
       "      <td>5325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green-glass</th>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metal</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper</th>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plastic</th>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoes</th>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trash</th>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white-glass</th>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Path\n",
       "Label            \n",
       "battery       945\n",
       "biological    985\n",
       "brown-glass   607\n",
       "cardboard     891\n",
       "clothes      5325\n",
       "green-glass   629\n",
       "metal         769\n",
       "paper        1050\n",
       "plastic       865\n",
       "shoes        1977\n",
       "trash         697\n",
       "white-glass   775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(files_data.groupby(\"Label\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ea69fe-25be-4a79-9d4e-a3ff3b0b81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_crop(image_array):\n",
    "    is_portrait = image_array.shape[0] >= image_array.shape[1]\n",
    "    large_side = image_array.shape[0] if is_portrait else image_array.shape[1]\n",
    "    small_side = image_array.shape[1] if is_portrait else image_array.shape[0]\n",
    "    crop_low = round(large_side / 2) - round(small_side / 2)\n",
    "    crop_high = round(large_side / 2) + (small_side - round(small_side / 2))\n",
    "    return image_array[crop_low:crop_high] if is_portrait\\\n",
    "           else image_array[:, crop_low:crop_high]\n",
    "\n",
    "\n",
    "def read_image(path, crop=True, resize=RESIZE):\n",
    "    try:\n",
    "        image_array = square_crop(cv2.imread(path))\n",
    "    except:\n",
    "        print(\"Error: \" + path)\n",
    "    image_array = cv2.resize(\n",
    "        image_array, dsize=[resize, resize], interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5097a893-3947-4c68-9a26-bf59833e43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'battery', 1: 'biological', 2: 'brown-glass', 3: 'cardboard', 4: 'clothes', 5: 'green-glass', 6: 'metal', 7: 'paper', 8: 'plastic', 9: 'shoes', 10: 'trash', 11: 'white-glass'}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = sorted(files_data[\"Label\"].unique())\n",
    "label_to_text = dict(zip(range(0, len(labels_unique)), labels_unique))\n",
    "text_to_label = dict(zip(labels_unique, range(0, len(labels_unique))))\n",
    "\n",
    "print(label_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1e381a-135d-4774-af0c-aed0dd211f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.9 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files_data_shuffle = files_data.sample(frac=1.0)\n",
    "\n",
    "label_limit = 9999\n",
    "counts = dict(zip(labels_unique, [0] * len(labels_unique)))\n",
    "\n",
    "inputs_list = list()\n",
    "labels_list = list()\n",
    "for file in files_data_shuffle.index:\n",
    "    label = files_data.at[file, \"Label\"]\n",
    "    \n",
    "    if counts[label] < label_limit: \n",
    "        inputs_list.append(\n",
    "            np.expand_dims(read_image(files_data.at[file, \"Path\"]), axis=0)\n",
    "        )\n",
    "        labels_list.append(text_to_label[label])\n",
    "        counts[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d59b21-2fa0-44f4-a32f-dbffc7e2bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate(inputs_list, axis=0) / 255\n",
    "labels = np.asarray(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce463a1-bd72-488c-b30a-fe005a10b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(inputs, labels, ratio=[9, 1]):\n",
    "    split_index = round(inputs.shape[0] * ratio[0] /sum(ratio))\n",
    "    train_inputs, test_inputs = inputs[:split_index], inputs[split_index:]\n",
    "    train_labels, test_labels = labels[:split_index], labels[split_index:]\n",
    "    print(f\"Train: {train_inputs.shape[0]}   Test: {test_inputs.shape[0]}\")\n",
    "    \n",
    "    return (train_inputs, train_labels), (test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8498d0ae-3076-408e-bec3-587fe3ba7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13964   Test: 1551\n"
     ]
    }
   ],
   "source": [
    "train_data_np, test_data_np = get_split(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aba0880-9e1a-4320-b9b6-38641932016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test data more balanced\n",
    "\n",
    "test_limit = 80\n",
    "test_counts = dict(zip(range(12), [0] * len(labels_unique)))\n",
    "\n",
    "test_inputs, test_labels = list(), list()\n",
    "for i, input_ in enumerate(test_data_np[0]):\n",
    "    label = test_data_np[1][i]\n",
    "    if test_counts[label] < test_limit:\n",
    "        test_inputs.append(np.expand_dims(input_, axis=0))\n",
    "        test_labels.append(label)\n",
    "        test_counts[label] += 1\n",
    "\n",
    "test_data_np = (np.concatenate(test_inputs, axis=0), np.asarray(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ed1288-9576-4fe9-828b-a17f6694a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs, self.labels = data  # expects tuple of numpy arrays\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def get_batch_inputs(self, idx):\n",
    "        # fetch a batch of inputs\n",
    "        return self.inputs[idx]\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        # fetch a batch of labels\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_inputs = self.get_batch_inputs(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c154f71d-3597-4e32-b5b7-cabd065d41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        # initialize layers\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, 1024)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.out = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # print(inputs.shape)\n",
    "        inputs = torch.movedim(inputs, -1, 1)\n",
    "        # forward propagation\n",
    "        inputs = self.resnet(inputs)\n",
    "        inputs = self.dense(inputs)\n",
    "        return self.out(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ffd7535-251a-4c22-908a-839aae143e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] battery: 5\n",
      "[1] biological: 4\n",
      "[2] brown-glass: 8\n",
      "[3] cardboard: 5\n",
      "[4] clothes: 0\n",
      "[5] green-glass: 7\n",
      "[6] metal: 6\n",
      "[7] paper: 4\n",
      "[8] plastic: 5\n",
      "[9] shoes: 2\n",
      "[10] trash: 7\n",
      "[11] white-glass: 6\n"
     ]
    }
   ],
   "source": [
    "data_count = np.asarray(files_data.groupby(\"Label\").count()[\"Path\"])\n",
    "aug_counts = (np.rint(data_count.max() / data_count) - 1).astype(int)\n",
    "for i, aug_count in enumerate(aug_counts):\n",
    "    print(f\"[{i}] {label_to_text[i]}: {aug_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41cbd01b-831c-44ff-abbf-4767c46205fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_resize(input_, size, center=True):\n",
    "    if center:\n",
    "        input_ = TF.center_crop(input_, size)\n",
    "    else:\n",
    "        input_ = TF.five_crop(input_, size)[random.randint(0, 5)]\n",
    "    return TF.resize(input_, [RESIZE, RESIZE])\n",
    "\n",
    "\n",
    "def data_aug(data):\n",
    "    inputs, labels = data\n",
    "    inputs = torch.movedim(torch.tensor(inputs), -1, 1)\n",
    "    rotate_crop = RESIZE - round(0.29289321881345254 * RESIZE)\n",
    "    crop_low = round(0.7 * RESIZE)\n",
    "    gaussian_high = round(0.05 * RESIZE)\n",
    "    \n",
    "    inputs_aug, labels_aug = list(), list()\n",
    "    for i, input_ in enumerate(tqdm(inputs)):\n",
    "        for j in range(aug_counts[labels[i]]):\n",
    "            random_bits = np.random.randint(2, size=8)\n",
    "            all_aug = not np.sum(random_bits)\n",
    "            # horizontal flip\n",
    "            if random_bits[0] or all_aug:\n",
    "                input_ = TF.hflip(input_)\n",
    "            # vertical flip\n",
    "            if random_bits[1] or all_aug:\n",
    "                input_ = TF.vflip(input_)\n",
    "            # rotation\n",
    "            if random_bits[2] or all_aug:\n",
    "                input_ = TF.rotate(input_, random.uniform(low=0.0, high=360.0))\n",
    "                input_ = crop_resize(input_, rotate_crop, center=True)\n",
    "            # hue\n",
    "            if random_bits[3] or all_aug:\n",
    "                input_ = TF.adjust_hue(\n",
    "                    input_, random.uniform(low=-0.2, high=0.2)\n",
    "                )\n",
    "            # saturation\n",
    "            if random_bits[4] or all_aug:\n",
    "                input_ = TF.adjust_saturation(\n",
    "                    input_, random.uniform(low=0.5, high=1.5)\n",
    "                )\n",
    "            # luminance (gamma)\n",
    "            if random_bits[5] or all_aug:\n",
    "                gamma = pow(random.normal(loc=1, scale=0.3), 2)\n",
    "                input_ = TF.adjust_gamma(input_, np.clip(gamma, 0.3, 1.7))\n",
    "            # gaussian blur\n",
    "            if random_bits[6] or all_aug:\n",
    "                input_ = TF.gaussian_blur(\n",
    "                    input_, random.randint(low=1, high=gaussian_high) * 2 - 1\n",
    "                )\n",
    "            # gaussian noise\n",
    "            if random_bits[7] or all_aug:\n",
    "                noise = torch.from_numpy(random.normal(\n",
    "                    loc=0.0,\n",
    "                    scale=random.uniform(low=0.0, high=0.1),\n",
    "                    size=input_.shape\n",
    "                ))\n",
    "                input_ = torch.add(input_, noise)\n",
    "            input_ = torch.clip(input_, 0.0, 1.0)\n",
    "            inputs_aug.append(torch.unsqueeze(input_, 0))\n",
    "            labels_aug.append(labels[i])\n",
    "    \n",
    "    inputs = torch.cat([inputs, torch.cat(inputs_aug, 0)], 0)\n",
    "    return (\n",
    "        torch.movedim(inputs, 1, -1).numpy(),\n",
    "        np.concatenate([labels, np.asarray(labels_aug)], axis=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc44fc9f-d4c2-451a-99b9-6037f94c0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the accuracy of one batch\n",
    "def evaluate_batch(batch_outputs, batch_labels):\n",
    "    batch_predictions = batch_outputs.argmax(axis=1)\n",
    "    return np.average((batch_predictions==batch_labels).cpu())\n",
    "\n",
    "\n",
    "# evaluate model with test data\n",
    "def evaluate(model, test_loader, loss, device=DEVICE):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracies, test_losses = [], []\n",
    "    for batch_inputs, batch_labels in iter(test_loader):\n",
    "        \n",
    "        batch_inputs = batch_inputs.type(torch.FloatTensor).to(device)  # CUDA\n",
    "        batch_labels = batch_labels.type(torch.int64).to(device)\n",
    "        \n",
    "        batch_outputs = model(batch_inputs)\n",
    "        test_accuracies.append(evaluate_batch(batch_outputs, batch_labels))\n",
    "        test_losses.append(loss(batch_outputs, batch_labels).cpu().item())\n",
    "    \n",
    "    del loss\n",
    "    \n",
    "    return np.average(test_accuracies), np.average(test_losses)\n",
    "\n",
    "\n",
    "# train for one epoch\n",
    "def train(model, train_data_np, test_data_np, loss, optimizer, device=DEVICE):\n",
    "    \n",
    "    if DATA_AUG:\n",
    "        train_data_np = data_aug(train_data_np)\n",
    "    train_data, test_data = Dataset(train_data_np), Dataset(test_data_np)\n",
    "    \n",
    "    train_loader = utils.data.DataLoader(\n",
    "        train_data, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    test_loader = utils.data.DataLoader(test_data, batch_size=1)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_accuracies, train_losses = list(), list()\n",
    "    for batch_inputs, batch_labels in tqdm(train_loader):\n",
    "        \n",
    "        batch_inputs = batch_inputs.type(torch.FloatTensor).to(device)  # CUDA\n",
    "        batch_labels = batch_labels.type(torch.int64).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs = model(batch_inputs)\n",
    "        batch_loss = loss(batch_outputs, batch_labels)\n",
    "        \n",
    "        train_accuracies.append(evaluate_batch(batch_outputs, batch_labels))\n",
    "        train_losses.append(batch_loss.cpu().item())\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del batch_inputs  # free CUDA memory (I think)\n",
    "        del batch_labels\n",
    "    \n",
    "    test_accuracy, test_loss = evaluate(model, test_loader, loss)\n",
    "    del loss\n",
    "    return np.average(train_accuracies), np.average(train_losses),\\\n",
    "           test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19708396-54a1-4dad-9ecc-fffb0334162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(3 * RESIZE * RESIZE, len(label_to_text.keys())).to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "outputs = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2a3e015-1583-4c10-ae90-36887ef0a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_name(name):\n",
    "    now = datetime.now()\n",
    "    time_string = now.strftime(\"%m%d_%H%M\")\n",
    "    return name + \"_\" + time_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9272702e-f164-4d51-9a47-a0edc3b7c9da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:16<00:00, 181.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [03:09<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.33253298566122486, 1.9319297369356196, 0.4994413407821229, 1.5552341625735673)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:15<00:00, 185.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.435537596095707, 1.6459998383395997, 0.553072625698324, 1.330761539833048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:20<00:00, 172.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.47370932128357945, 1.543314242415491, 0.5519553072625698, 1.3745200985701584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:34<00:00, 147.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.503119398915954, 1.46013837737659, 0.5586592178770949, 1.3520910876889274)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:35<00:00, 146.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5221381159842792, 1.4002034396327014, 0.5932960893854748, 1.2737352651929623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:44<00:00, 133.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5474648008983329, 1.3274510374153239, 0.6167597765363129, 1.2517553084814146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:48<00:00, 128.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5631630198885722, 1.2795418023012808, 0.6268156424581005, 1.215572213744464)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13964/13964 [01:59<00:00, 116.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 454/454 [02:56<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.578973597974432, 1.2328403139166895, 0.617877094972067, 1.2396404721822736)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for epoch in range(4):\n",
    "        output = train(model, train_data_np, test_data_np, loss, optimizer)\n",
    "        outputs.append(output)\n",
    "        print(output)\n",
    "    torch.save(model.state_dict(), \"models/\" + generate_model_name(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd6b930-21e3-4c22-b745-3fd11dc6de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4994413407821229, 0.553072625698324, 0.5519553072625698, 0.5586592178770949, 0.5932960893854748, 0.6167597765363129, 0.6268156424581005, 0.617877094972067, "
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output[2], end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c9fd6-fddd-45dc-9a12-d1725c03ab2b",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d7f74-a232-4a8d-9c17-5183ef0bc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/model_0416_1135\"\n",
    "\n",
    "model_load = Model(3 * RESIZE & RESIZE, 12)\n",
    "model_load.load_state_dict(torch.load(model_path))\n",
    "model_load.eval()\n",
    "print(f\"Model at \\\"{model_path}\\\" loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e246b-5093-418b-8708-5ecd6daf8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = utils.data.DataLoader(\n",
    "    Dataset(test_data_np), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Test accuracy: {evaluate(model, test_loader, loss)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ee2a5-3239-4952-8dc7-94d025dfa664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
